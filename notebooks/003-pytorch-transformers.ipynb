{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Experimenting with PyTorch and Transformers\n",
    "\n",
    "This notebook is a playground for experimenting with PyTorch and Transformers. The goal is to get a better understanding of how to use these libraries and how to apply them to different tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f2c43569c9fc015"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eb3709ae7f951c75",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basics of Transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a16842ba1fa4cbce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2b459dec3770a0a3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Architecture of a Transformer Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e46181467b46e7a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ea239074f0947fc1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Multi-Head Attention Sublayer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ea6f3f90e10541e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Initialize input vectors\n",
    "x = torch.tensor([[3, 0, 0], [1, 1, 0],[5, 6, 1],[2, 2, 4]]).reshape(4,3)\n",
    "print(\"Input \", x)\n",
    "print(\"Number of inputs:\", x.shape[0])\n",
    "print(\"d-model:\", x.shape[1])\n",
    "\n",
    "# Initialize the query, weight and key matrix\n",
    "weight_key = torch.tensor([[0, 0, 0, 0], [2, 1, 1, 2], [0, 1, 2, 0]]).reshape(3,4)\n",
    "print(\"\\nkey weight matrix\", weight_key)\n",
    "\n",
    "weight_query = torch.tensor([[1, 0, 1, 0],[0, 2, 1, 0],[0, 3, 2, 1]]).reshape(3,4)\n",
    "print(\"\\nquery weight matrix\", weight_query)\n",
    "\n",
    "weight_value = torch.tensor([[0, 2, 3, 1],[0, 3, 2, 4],[1, 0, 5, 2]]).reshape(3,4)\n",
    "print(\"\\nvalue weight matrix\", weight_value)\n",
    "\n",
    "# Perform matrix multiplication with the query,key,value weight matrices with the input vectors\n",
    "k = x @ weight_key # The key weight matrix\n",
    "q = x @ weight_query # The query weight matrix\n",
    "v = x @ weight_value # The value weight matrix\n",
    "\n",
    "# scaling attention scores\n",
    "k_d = 2 #sqrt root of d_k = 4\n",
    "attention_scores = (q @ k.transpose(0, 1)) / k_d\n",
    "print(\"\\nattention_scores\", attention_scores)\n",
    "\n",
    "#Multiplying attention scores with value matrix for one input (the number of att scores can be modified here!)\n",
    "attention_values = []\n",
    "for i in range(x.shape[0]): # loop over num features\n",
    "    for j in range(x.shape[1]): # loop over attention heads\n",
    "        attention = attention_scores[i][j] * v[j]\n",
    "        attention_values.append(attention)\n",
    "print(\"\\nattention_values:\\n\", attention_values)\n",
    "\n",
    "#add attention scores to get one row of output matrix. Repeat for other inputs (the num of output matrix rows can be modified here!)\n",
    "o1=None\n",
    "o2=None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c69ea2821ba7bc67",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21ba85db939814c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6350934b18cd1ddb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-Tuning BERT Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd0a5c54a38b2606"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c1077efe0b1fdf16",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretraining RoBERTa from Scratch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ad23bc3f8d081fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ec771f0acb760932",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Downstream NLP Tasks using Transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6012f015272d8e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "11cd62f1cae69206",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Translation with Transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32223931d58c92a4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "94531e7a9c460ce7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Ries of Transformers with GPT-3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d41858fb4b46b34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6043897d7beda652",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text summarization with Transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3daf9e3de2dedbb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "313bc9ae4ae55e86",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizers and Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8228e7e1fc935768"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c680c675bda54a74",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic Labeling with BERT-based Transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57cc03407c8e34e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21aabf2ba89a4853",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question Answering with Transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3cca1df3341375a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "86b94a6b5163f6e1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis with Transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b54e0d7a3ba4cf6d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c7872fbe50eba6a1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fake News detection with Transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cd0abc0d8f80b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f9c41f8f1c2ca073",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Interpreting Black-Box Transformer Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81c25944760c212e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0a96a589a2149f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task Agnostic Transformer Models (non NLP)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83bfda01921e1feb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "81d3fd67d1e2910a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer Models as Copilots"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d676fd2b0c888acd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ea6ff6350982d61f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4090c7f4f957069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Questions and Answers\n",
    "\n",
    "**Q: What are attention heads in transformer models?**\n",
    "A: Attention heads are the individual parallel pathways through which the model processes information, allowing the model to focus on different parts of the input data, learning different patterns and relationships in parallel.\n",
    "\n",
    "**Q: Why is/was the attention mechanism so important in the development of transformer models?**\n",
    "A: It allows the parallel processing of input data (different attention heads can run on different GPUs), which is more efficient than the sequential processing of RNNs and LSTMs. This parallel processing is what allows transformer models to scale to larger datasets and more complex tasks.\n",
    " \n",
    "Q: Can transformer models only be applied to NLP tasks?\n",
    "A: No, transformer models can be applied to a wide range of tasks, including image recognition, speech recognition, and other tasks that involve processing sequences of data.\n",
    "\n",
    "Q: What is the basic architecture of a transformer architecture?\n",
    "Q: What are foundation models?\n",
    "Q: What are the different components in a standard transformer model?\n",
    "Q: What are some techniques used to train transformer models?\n",
    "Q: What are some techniques used to fine-tune transformer models?\n",
    "Q: What were the steps to pretrain RoBERTa models?\n",
    "Q: How can transformer models be used for machine translation (NLP)?\n",
    "Q: How can transformer models be used for image recognition?\n",
    "Q: Explain the similarities and differences between OpenAI's GPT-2 and GPR-3\n",
    "Q: Explain the concept of T5 transformer model\n",
    "Q: Explain the architecture of T5 transformer model\n",
    "Q: How is the quality of data improved in/for T5 transformer model?\n",
    "Q: Explain how transformer models are able to \"understand\" the context of text.\n",
    "Q: Explain how transformer models are able to \"understand\" long text and display reasoning skills\n",
    "Q: By which methods have transformers improved sentiment analysis?\n",
    "Q: How can transformers be used to understand different perspectives in text?\n",
    "Q: What are some hidden details in transformer models that are not often discussed?\n",
    "Q: What are some properties of advanced transformer models?\n",
    "Q: What are some different transformers for vision tasks?\n",
    "Q: How are vision transformers tested, for example for image generation?\n",
    "  \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11aa29d3550d4740"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
