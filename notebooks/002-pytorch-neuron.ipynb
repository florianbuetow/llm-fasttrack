{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training a single neuron using PyTorch\n",
    "\n",
    "This is an example of how to implement a neuron and its training using PyTorch. I am using  mini-batch gradient descent (aka stochastic gradient descent) and sigmoid as the activation function. I am trahing the neuron to predict the probability of a binary label.\n",
    "\n",
    "I am using the classic [Anderson's Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) containing samples of three types of flowers and their petal length and width. It is a simple dataset that is often used in introductions to machine learning to demonstrate classification algorithms. Here I am using it to demonstrate how to train a neuron using PyTorch. To use it to train a single neuron I am removing one of the flowers and two of the four features from the dataset since it is generally not feasible to train a single neuron to classify three classes and two features will be enough to demonstrate that the neuron can be trained. \n",
    "\n",
    "Contents:\n",
    "\n",
    "1. Loading the dataset using scikit-learn\n",
    "2. Exploring the dataset and removing unwanted features and classes\n",
    "3. Converting the dataset into pytorch tensors and creating dataloaders\n",
    "4. Implementation of a neuron and its training using mini-batch gradient descent with PyTorch\n",
    "5. Training and evaluating the neuron using accuracy and confusion matrix and visualising the decision boundary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e5d8980da218212"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Load the Iris dataset using scikit-learn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e978db8179dcd2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# I am using the intel optimized version of sklearn if you don't have an Intel CPU, you can remove the following lines:\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c56b8e7169f1448f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "print(\"The dataset contains {} samples with {} features and the labels {}.\".format(iris.data.shape[0], iris.data.shape[1], iris.target_names))\n",
    "\n",
    "# Extract the feature data and target labels\n",
    "X_original = iris.data\n",
    "y_original = iris.target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12448d9396a984cc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Visualize the Dataset with Seaborn\n",
    "\n",
    "The dataset contains three species of flowers and four features for each sample. The features are the sepal length, sepal width, petal length, and petal width. Lets explore a pairplot that shows the relationship between each pair of two features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45767436054a843e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# For seaborn it is convenient to use a pandas DataFrame\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) \n",
    "iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "sns.pairplot(iris_df, hue='species', markers=[\"o\", \"s\", \"D\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f9d52fef0f2a42",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "I decided to remove the Versicolor class from the dataset since it is generally not feasible to train a single neuron to classify three classes. The remaining samples of Setosa and Virginica should allow us to see a nice decision boundary between the two after we've trained the neuron. I also decided to remove two features so tht the remaining ones are the petal length and petal width. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed1328da082d5992"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Get the index that corresponds to 'virginica'\n",
    "versicolor_idx = np.where(iris.target_names == 'virginica')[0][0]\n",
    "# Find indices where the target label is not 'virginica'\n",
    "non_versicolor_idx = y_original != versicolor_idx\n",
    "# Filter out 'virginica' samples\n",
    "X = X_original[non_versicolor_idx]\n",
    "y = y_original[non_versicolor_idx]\n",
    "# And remember the remaining labels\n",
    "flower_labels = iris.target_names[:2] \n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# remove two columns from the feature matrix\n",
    "X = X[:, 2:]  # Keep only petal length and petal width\n",
    "feature_names = feature_names[2:]  # Keep only petal length and petal width"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "addb4286b6f86efc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "iris_df = pd.DataFrame(X, columns=feature_names)\n",
    "iris_df['species'] = pd.Categorical.from_codes(y, flower_labels)\n",
    "# sns scatterplot\n",
    "sns.scatterplot(data=iris_df, x=feature_names[0], y=feature_names[1], hue='species', style='species')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "835a7d74b8e7a533",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert the dataset to PyTorch tensors and create dataloaders\n",
    "\n",
    "To train the neuron, we need to convert the data to PyTorch tensors and create datasets and dataloaders. I am using 80% of the data for training and 20% for testing. I am using a batch size of 10. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4de51165e1a87bb8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# First, convert the whole dataset to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Determine split sizes\n",
    "total_count = len(dataset)\n",
    "test_count = int(0.2 * total_count)\n",
    "train_count = total_count - test_count\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_count, test_count])\n",
    "\n",
    "# Create PyTorch dataloaders\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a427238a3c97302",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation of the neuron, training and evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f04bf3ec607da1cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self):\n",
    "        np.random.seed(42)\n",
    "        # Four weights and one bias term\n",
    "        self.weights = torch.randn(3, 1)  # This already uses mean=0 and std=1 by default\n",
    "        self.loss = []\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return torch.sigmoid(x)\n",
    "        \n",
    "    def train(self, train_loader, learning_rate=0.01, epochs=100):\n",
    "        for _ in range(epochs):\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                # x_batch has the shape (batch_size, 2) \n",
    "                # but we need to add a bias term by adding a column of ones \n",
    "                x_batch = torch.cat((x_batch, torch.ones((x_batch.shape[0], 1))), 1)                \n",
    "                \n",
    "                # For the forward pass we simply multiply the batch of features with the weights vector\n",
    "                # and then apply the sigmoid activation function to get the predictions                \n",
    "                y_hat = self.sigmoid(torch.matmul(x_batch, self.weights))                \n",
    "                # due to the matrix multiplication, y_hat is now a two-dimensional tensor\n",
    "                # with shape (10, 1) where the 2nd dimension is not used. Let's remove it. \n",
    "                y_hat = y_hat.squeeze() # y_hat is now a one-dimensional tensor of length batch_size\n",
    "                \n",
    "                # Calculate the loss by comparing the predictions with the actual labels\n",
    "                errors = y_hat - y_batch # gives a one-dimensional tensor of length batch_size\n",
    "                \n",
    "                # Calculate the gradient of the loss with respect to the weights\n",
    "                loss_gradient = torch.matmul(errors, x_batch) / x_batch.size(0) # 1-d tensor of length 3 (=number of features + bias)     \n",
    "                \n",
    "                # Update weights\n",
    "                weight_update = learning_rate * loss_gradient\n",
    "                self.weights -= weight_update.view(-1, 1) # weight_update is a 1-d tensor of length 3 (=number of features + bias)\n",
    "                self.loss.append(torch.mean(errors ** 2).item())\n",
    "                \n",
    "    def eval(self, test_loader):\n",
    "        total_predictions = correct_predictions = 0\n",
    "        confusion_matrix = torch.zeros(2, 2)  # Assuming binary classification\n",
    "\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            y_hat = self.predict(x_batch)\n",
    "            y_hat = torch.round(y_hat)  # Round to get class predictions\n",
    "            total_predictions += test_loader.batch_size\n",
    "\n",
    "            for i in range(len(y_batch)):\n",
    "                if y_batch[i] == y_hat[i]: correct_predictions += 1\n",
    "                confusion_matrix[y_batch[i], y_hat[i]] += 1\n",
    "\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        return accuracy, confusion_matrix\n",
    "        \n",
    "    # Return the corresponding 0 or 1 label.\n",
    "    def predict(self, features):\n",
    "        features = torch.cat((features, torch.ones((features.shape[0], 1), dtype=torch.float32)), 1)\n",
    "        prediction = self.sigmoid(features @ self.weights)\n",
    "        prediction = prediction.squeeze()\n",
    "        prediction = torch.round(prediction)\n",
    "        prediction = prediction.type(torch.long)\n",
    "        return prediction\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e04794156d6e989",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the neuron on the data and visualise the training loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbcf1c6315dd53f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train the neuron\n",
    "neuron = Neuron()\n",
    "neuron.train(train_loader, learning_rate=0.05, epochs=100)\n",
    "# Plot the loss\n",
    "plt.plot(neuron.loss)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('MSE Loss')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "829bce366fdb448e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0e664c03e2c997"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now evaluate the neuron on the test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3ac39ced13f3647"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "acc, confusion_matrix = neuron.eval(test_loader)\n",
    "print(f'Accuracy: {acc * 100:.2f}%')\n",
    "\n",
    "# visualise the confusion matrix\n",
    "import seaborn as sns\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=flower_labels, yticklabels=flower_labels)  \n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dda0b4498cd0cad9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# And visualise the decision boundary\n",
    "iris_df = pd.DataFrame(X, columns=feature_names)\n",
    "iris_df['species'] = pd.Categorical.from_codes(y, flower_labels)\n",
    "# sns scatterplot\n",
    "sns.scatterplot(data=iris_df, x=feature_names[0], y=feature_names[1], hue='species', style='species')\n",
    "\n",
    "# now add the decision boundary of the neuron based on the predictoins of the neuron\n",
    "x_min, x_max = plt.xlim()\n",
    "y_min, y_max = plt.ylim()\n",
    "\n",
    "# Create a meshgrid using torch.meshgrid\n",
    "x_range = torch.linspace(x_min, x_max, 100)\n",
    "y_range = torch.linspace(y_min, y_max, 100)\n",
    "xx, yy = torch.meshgrid(x_range, y_range, indexing='ij')\n",
    "\n",
    "Z = neuron.predict(torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Decision Boundary')\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34cf1292ae404791",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the neuron has learned to separate the two classes quite well. The decision boundary is a straight line (it appears jittery because we are using a grid of samples to approximate it), which is expected since we are using a single neuron. The accuracy of the neuron is 100% on the test data. This is expected since the dataset is simple and the neuron has enough capacity to learn the decision boundary."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcc333e9cc13c879"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
